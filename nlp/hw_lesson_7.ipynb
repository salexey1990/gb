{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = '/home/alex/Downloads/nlp/lesson_7/data/rus-eng/rus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем из текстов токены и делаем pne-hot вектора на каждый токен\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alex/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/alex/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/alex/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 1.1220 - accuracy: 0.7734 - val_loss: 0.9151 - val_accuracy: 0.7577\n",
      "Epoch 2/30\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.7379 - accuracy: 0.8011 - val_loss: 0.7835 - val_accuracy: 0.7916\n",
      "Epoch 3/30\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.7258 - accuracy: 0.8150 - val_loss: 0.7483 - val_accuracy: 0.8000\n",
      "Epoch 4/30\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.6134 - accuracy: 0.8368 - val_loss: 0.6790 - val_accuracy: 0.8150\n",
      "Epoch 5/30\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 0.5679 - accuracy: 0.8442 - val_loss: 0.6435 - val_accuracy: 0.8212\n",
      "Epoch 6/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.5393 - accuracy: 0.8488 - val_loss: 0.6179 - val_accuracy: 0.8237\n",
      "Epoch 7/30\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.5187 - accuracy: 0.8526 - val_loss: 0.5970 - val_accuracy: 0.8291\n",
      "Epoch 8/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.5016 - accuracy: 0.8564 - val_loss: 0.5779 - val_accuracy: 0.8369\n",
      "Epoch 9/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.4866 - accuracy: 0.8599 - val_loss: 0.5657 - val_accuracy: 0.8353\n",
      "Epoch 10/30\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.4754 - accuracy: 0.8623 - val_loss: 0.5555 - val_accuracy: 0.8397\n",
      "Epoch 11/30\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.4638 - accuracy: 0.8651 - val_loss: 0.5430 - val_accuracy: 0.8429\n",
      "Epoch 12/30\n",
      "8000/8000 [==============================] - 35s 4ms/step - loss: 0.4538 - accuracy: 0.8675 - val_loss: 0.5320 - val_accuracy: 0.8458\n",
      "Epoch 13/30\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.4437 - accuracy: 0.8701 - val_loss: 0.5226 - val_accuracy: 0.8484\n",
      "Epoch 14/30\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.4347 - accuracy: 0.8725 - val_loss: 0.5133 - val_accuracy: 0.8504\n",
      "Epoch 15/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.4254 - accuracy: 0.8756 - val_loss: 0.5055 - val_accuracy: 0.8537\n",
      "Epoch 16/30\n",
      "8000/8000 [==============================] - 29s 4ms/step - loss: 0.4161 - accuracy: 0.8783 - val_loss: 0.5003 - val_accuracy: 0.8561\n",
      "Epoch 17/30\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.4083 - accuracy: 0.8804 - val_loss: 0.4938 - val_accuracy: 0.8560\n",
      "Epoch 18/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.4006 - accuracy: 0.8822 - val_loss: 0.4896 - val_accuracy: 0.8577\n",
      "Epoch 19/30\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.4234 - accuracy: 0.8776 - val_loss: 0.4974 - val_accuracy: 0.8556\n",
      "Epoch 20/30\n",
      "8000/8000 [==============================] - 30s 4ms/step - loss: 0.3981 - accuracy: 0.8831 - val_loss: 0.4852 - val_accuracy: 0.8598\n",
      "Epoch 21/30\n",
      "8000/8000 [==============================] - 38s 5ms/step - loss: 0.3871 - accuracy: 0.8864 - val_loss: 0.4775 - val_accuracy: 0.8611\n",
      "Epoch 22/30\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.3790 - accuracy: 0.8886 - val_loss: 0.4700 - val_accuracy: 0.8648\n",
      "Epoch 23/30\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.3714 - accuracy: 0.8910 - val_loss: 0.4656 - val_accuracy: 0.8659\n",
      "Epoch 24/30\n",
      "8000/8000 [==============================] - 28s 4ms/step - loss: 0.3653 - accuracy: 0.8926 - val_loss: 0.4626 - val_accuracy: 0.8661\n",
      "Epoch 25/30\n",
      "8000/8000 [==============================] - 28s 3ms/step - loss: 0.3582 - accuracy: 0.8948 - val_loss: 0.4569 - val_accuracy: 0.8691\n",
      "Epoch 26/30\n",
      "8000/8000 [==============================] - 31s 4ms/step - loss: 0.3518 - accuracy: 0.8966 - val_loss: 0.4520 - val_accuracy: 0.8699\n",
      "Epoch 27/30\n",
      "8000/8000 [==============================] - 40s 5ms/step - loss: 0.3447 - accuracy: 0.8989 - val_loss: 0.4493 - val_accuracy: 0.8697\n",
      "Epoch 28/30\n",
      "8000/8000 [==============================] - 37s 5ms/step - loss: 0.3390 - accuracy: 0.9005 - val_loss: 0.4449 - val_accuracy: 0.8716\n",
      "Epoch 29/30\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.3321 - accuracy: 0.9025 - val_loss: 0.4439 - val_accuracy: 0.8719\n",
      "Epoch 30/30\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.3257 - accuracy: 0.9046 - val_loss: 0.4398 - val_accuracy: 0.8750\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Он всёрна.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Он всёрна.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Он всёрна.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Он всёрна.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Он всёрна.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Не пойдё!\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Не пойдё!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Не справитевайте!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Не справитевайте!\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Кто стал?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вы стободе!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вы стободе!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вы стободе!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вы стободе!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вы стободе!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Вы стободе!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Постань!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Постань!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Помоги!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Помоги!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Помоги!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Простовойте!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Простовойте!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Простовойте!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Простовойте!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Прекратите!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Прекратите!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Прекратите!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Подожди!\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подожди.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подожди.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подожди.\n",
      "\n",
      "-\n",
      "Input sentence: Do it.\n",
      "Decoded sentence: Не справитей.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Прекрите!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Прекрите!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Прекрите!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Прекрите!\n",
      "\n",
      "-\n",
      "Input sentence: Hurry!\n",
      "Decoded sentence: Привесь!\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я пострялся.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я пострялся.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я пострялся.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я пострялся.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я была ставал.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я была ставал.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я была ставал.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я была ставал.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я была ставал.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я была ставал.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я пострялся.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я пострялся.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я пострялся.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я пострялся.\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Она теста!\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Подождитесь!\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Подождитесь!\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Подождитесь!\n",
      "\n",
      "-\n",
      "Input sentence: Shoot!\n",
      "Decoded sentence: Покожите!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Посмотрите!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Посмотрите!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Посмотрите!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Посмотрите!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Посмотрите!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Посмотрите!\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Останьтесь!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Поводите!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Поводите!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Поводите!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Поводите!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: Поводите!\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Поставьте это.\n",
      "\n",
      "-\n",
      "Input sentence: Eat up.\n",
      "Decoded sentence: Отайте его.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Привесь!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Привесь!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Привесь!\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Привесь!\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Привесь!\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Привесь!\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Привесь!\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите серь.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Простите!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Постовите!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Постовите!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Постовите!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Постовите!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае побуквенной генерации получается совсем плохое качество перевода. Много граммарических ошибок, и, зачастую, перевод крайне далёк от значения слова\n",
    "\n",
    "Input sentence: Hello!\n",
    "Decoded sentence: Прекрите!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "data_path = '/home/alex/Downloads/nlp/lesson_7/data/rus-eng/rus.txt'\n",
    "num_samples = 10000\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(preprocess_sentence(input_text))\n",
    "    target_texts.append(preprocess_sentence(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "    \n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alex/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1 Loss 0.1086\n",
      "Epoch 2 Loss 0.0312\n",
      "Epoch 3 Loss 0.0241\n",
      "Epoch 4 Loss 0.0209\n",
      "Epoch 5 Loss 0.0201\n",
      "Epoch 6 Loss 0.0179\n",
      "Epoch 7 Loss 0.0169\n",
      "Epoch 8 Loss 0.0159\n",
      "Epoch 9 Loss 0.0162\n",
      "Epoch 10 Loss 0.0147\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые украденные функции для оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "#     plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Input: <start> good morning <end>\n",
      "Predicted translation: ! <end> \n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "translate(u'good morning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hello <end>\n",
      "Predicted translation: ! <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучить не удалось, на любую фразу ответ один \" !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "    \n",
    "    \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = len(inp_lang_tokenizer.index_word) + 2\n",
    "target_vocab_size = len(targ_lang_tokenizer.index_word) + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "  \n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')\n",
    "\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "  \n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.9616 Accuracy 0.0508\n",
      "Epoch 1 Batch 50 Loss 1.8431 Accuracy 0.1759\n",
      "Epoch 1 Batch 100 Loss 1.4395 Accuracy 0.2280\n",
      "Epoch 1 Loss 1.2845 Accuracy 0.2601\n",
      "Epoch 2 Batch 0 Loss 0.5668 Accuracy 0.4297\n",
      "Epoch 2 Batch 50 Loss 0.2566 Accuracy 0.4710\n",
      "Epoch 2 Batch 100 Loss 0.2043 Accuracy 0.4792\n",
      "Epoch 2 Loss 0.1876 Accuracy 0.4815\n",
      "Epoch 3 Batch 0 Loss 0.1216 Accuracy 0.4922\n",
      "Epoch 3 Batch 50 Loss 0.1194 Accuracy 0.4916\n",
      "Epoch 3 Batch 100 Loss 0.1209 Accuracy 0.4913\n",
      "Epoch 3 Loss 0.1170 Accuracy 0.4916\n",
      "Epoch 4 Batch 0 Loss 0.1099 Accuracy 0.4961\n",
      "Epoch 4 Batch 50 Loss 0.1068 Accuracy 0.4921\n",
      "Epoch 4 Batch 100 Loss 0.1060 Accuracy 0.4915\n",
      "Epoch 4 Loss 0.1025 Accuracy 0.4920\n",
      "Epoch 5 Batch 0 Loss 0.0966 Accuracy 0.4922\n",
      "Epoch 5 Batch 50 Loss 0.0885 Accuracy 0.4920\n",
      "Epoch 5 Batch 100 Loss 0.0887 Accuracy 0.4919\n",
      "Epoch 5 Loss 0.0866 Accuracy 0.4922\n",
      "Epoch 6 Batch 0 Loss 0.0635 Accuracy 0.5000\n",
      "Epoch 6 Batch 50 Loss 0.0815 Accuracy 0.4920\n",
      "Epoch 6 Batch 100 Loss 0.0807 Accuracy 0.4918\n",
      "Epoch 6 Loss 0.0780 Accuracy 0.4920\n",
      "Epoch 7 Batch 0 Loss 0.0688 Accuracy 0.5000\n",
      "Epoch 7 Batch 50 Loss 0.0684 Accuracy 0.4932\n",
      "Epoch 7 Batch 100 Loss 0.0708 Accuracy 0.4936\n",
      "Epoch 7 Loss 0.0686 Accuracy 0.4938\n",
      "Epoch 8 Batch 0 Loss 0.0780 Accuracy 0.5039\n",
      "Epoch 8 Batch 50 Loss 0.0636 Accuracy 0.4950\n",
      "Epoch 8 Batch 100 Loss 0.0612 Accuracy 0.4956\n",
      "Epoch 8 Loss 0.0603 Accuracy 0.4956\n",
      "Epoch 9 Batch 0 Loss 0.0746 Accuracy 0.5000\n",
      "Epoch 9 Batch 50 Loss 0.0588 Accuracy 0.4953\n",
      "Epoch 9 Batch 100 Loss 0.0583 Accuracy 0.4963\n",
      "Epoch 9 Loss 0.0584 Accuracy 0.4962\n",
      "Epoch 10 Batch 0 Loss 0.0757 Accuracy 0.5000\n",
      "Epoch 10 Batch 50 Loss 0.0552 Accuracy 0.4957\n",
      "Epoch 10 Batch 100 Loss 0.0537 Accuracy 0.4963\n",
      "Epoch 10 Loss 0.0526 Accuracy 0.4965\n",
      "Epoch 11 Batch 0 Loss 0.0790 Accuracy 0.5039\n",
      "Epoch 11 Batch 50 Loss 0.0538 Accuracy 0.4963\n",
      "Epoch 11 Batch 100 Loss 0.0523 Accuracy 0.4968\n",
      "Epoch 11 Loss 0.0518 Accuracy 0.4968\n",
      "Epoch 12 Batch 0 Loss 0.0750 Accuracy 0.5039\n",
      "Epoch 12 Batch 50 Loss 0.0517 Accuracy 0.4958\n",
      "Epoch 12 Batch 100 Loss 0.0497 Accuracy 0.4968\n",
      "Epoch 12 Loss 0.0493 Accuracy 0.4966\n",
      "Epoch 13 Batch 0 Loss 0.0616 Accuracy 0.5039\n",
      "Epoch 13 Batch 50 Loss 0.0548 Accuracy 0.4947\n",
      "Epoch 13 Batch 100 Loss 0.0500 Accuracy 0.4964\n",
      "Epoch 13 Loss 0.0496 Accuracy 0.4964\n",
      "Epoch 14 Batch 0 Loss 0.0626 Accuracy 0.5000\n",
      "Epoch 14 Batch 50 Loss 0.0484 Accuracy 0.4959\n",
      "Epoch 14 Batch 100 Loss 0.0514 Accuracy 0.4965\n",
      "Epoch 14 Loss 0.0497 Accuracy 0.4966\n",
      "Epoch 15 Batch 0 Loss 0.0592 Accuracy 0.5039\n",
      "Epoch 15 Batch 50 Loss 0.0441 Accuracy 0.4963\n",
      "Epoch 15 Batch 100 Loss 0.0445 Accuracy 0.4971\n",
      "Epoch 15 Loss 0.0447 Accuracy 0.4970\n",
      "Epoch 16 Batch 0 Loss 0.0630 Accuracy 0.5039\n",
      "Epoch 16 Batch 50 Loss 0.0550 Accuracy 0.4948\n",
      "Epoch 16 Batch 100 Loss 0.0547 Accuracy 0.4960\n",
      "Epoch 16 Loss 0.0533 Accuracy 0.4960\n",
      "Epoch 17 Batch 0 Loss 0.0693 Accuracy 0.5039\n",
      "Epoch 17 Batch 50 Loss 0.0495 Accuracy 0.4959\n",
      "Epoch 17 Batch 100 Loss 0.0459 Accuracy 0.4974\n",
      "Epoch 17 Loss 0.0469 Accuracy 0.4972\n",
      "Epoch 18 Batch 0 Loss 0.0749 Accuracy 0.5039\n",
      "Epoch 18 Batch 50 Loss 0.0489 Accuracy 0.4962\n",
      "Epoch 18 Batch 100 Loss 0.0458 Accuracy 0.4973\n",
      "Epoch 18 Loss 0.0443 Accuracy 0.4973\n",
      "Epoch 19 Batch 0 Loss 0.0700 Accuracy 0.5000\n",
      "Epoch 19 Batch 50 Loss 0.0438 Accuracy 0.4976\n",
      "Epoch 19 Batch 100 Loss 0.0460 Accuracy 0.4975\n",
      "Epoch 19 Loss 0.0493 Accuracy 0.4968\n",
      "Epoch 20 Batch 0 Loss 0.0665 Accuracy 0.5000\n",
      "Epoch 20 Batch 50 Loss 0.0466 Accuracy 0.4966\n",
      "Epoch 20 Batch 100 Loss 0.0453 Accuracy 0.4978\n",
      "Epoch 20 Loss 0.0442 Accuracy 0.4976\n",
      "Epoch 21 Batch 0 Loss 0.0818 Accuracy 0.4961\n",
      "Epoch 21 Batch 50 Loss 0.0417 Accuracy 0.4969\n",
      "Epoch 21 Batch 100 Loss 0.0454 Accuracy 0.4978\n",
      "Epoch 21 Loss 0.0454 Accuracy 0.4974\n",
      "Epoch 22 Batch 0 Loss 0.0671 Accuracy 0.5000\n",
      "Epoch 22 Batch 50 Loss 0.0498 Accuracy 0.4961\n",
      "Epoch 22 Batch 100 Loss 0.0489 Accuracy 0.4971\n",
      "Epoch 22 Loss 0.0472 Accuracy 0.4975\n",
      "Epoch 23 Batch 0 Loss 0.0759 Accuracy 0.5039\n",
      "Epoch 23 Batch 50 Loss 0.0506 Accuracy 0.4969\n",
      "Epoch 23 Batch 100 Loss 0.0475 Accuracy 0.4978\n",
      "Epoch 23 Loss 0.0513 Accuracy 0.4973\n",
      "Epoch 24 Batch 0 Loss 0.1614 Accuracy 0.4883\n",
      "Epoch 24 Batch 50 Loss 0.0723 Accuracy 0.4918\n",
      "Epoch 24 Batch 100 Loss 0.0771 Accuracy 0.4921\n",
      "Epoch 24 Loss 0.0732 Accuracy 0.4924\n",
      "Epoch 25 Batch 0 Loss 0.1306 Accuracy 0.4961\n",
      "Epoch 25 Batch 50 Loss 0.0692 Accuracy 0.4927\n",
      "Epoch 25 Batch 100 Loss 0.0719 Accuracy 0.4927\n",
      "Epoch 25 Loss 0.0704 Accuracy 0.4935\n",
      "Epoch 26 Batch 0 Loss 0.0491 Accuracy 0.5000\n",
      "Epoch 26 Batch 50 Loss 0.0605 Accuracy 0.4954\n",
      "Epoch 26 Batch 100 Loss 0.0551 Accuracy 0.4967\n",
      "Epoch 26 Loss 0.0520 Accuracy 0.4971\n",
      "Epoch 27 Batch 0 Loss 0.0639 Accuracy 0.5039\n",
      "Epoch 27 Batch 50 Loss 0.0448 Accuracy 0.4971\n",
      "Epoch 27 Batch 100 Loss 0.0474 Accuracy 0.4975\n",
      "Epoch 27 Loss 0.0451 Accuracy 0.4976\n",
      "Epoch 28 Batch 0 Loss 0.0827 Accuracy 0.5000\n",
      "Epoch 28 Batch 50 Loss 0.0459 Accuracy 0.4975\n",
      "Epoch 28 Batch 100 Loss 0.0458 Accuracy 0.4978\n",
      "Epoch 28 Loss 0.0452 Accuracy 0.4977\n",
      "Epoch 29 Batch 0 Loss 0.0591 Accuracy 0.5000\n",
      "Epoch 29 Batch 50 Loss 0.0449 Accuracy 0.4976\n",
      "Epoch 29 Batch 100 Loss 0.0467 Accuracy 0.4982\n",
      "Epoch 29 Loss 0.0503 Accuracy 0.4970\n",
      "Epoch 30 Batch 0 Loss 0.0810 Accuracy 0.4961\n",
      "Epoch 30 Batch 50 Loss 0.0609 Accuracy 0.4943\n",
      "Epoch 30 Batch 100 Loss 0.0561 Accuracy 0.4963\n",
      "Epoch 30 Loss 0.0539 Accuracy 0.4967\n",
      "Epoch 31 Batch 0 Loss 0.0467 Accuracy 0.5039\n",
      "Epoch 31 Batch 50 Loss 0.0561 Accuracy 0.4956\n",
      "Epoch 31 Batch 100 Loss 0.0560 Accuracy 0.4972\n",
      "Epoch 31 Loss 0.0701 Accuracy 0.4957\n",
      "Epoch 32 Batch 0 Loss 0.0656 Accuracy 0.5000\n",
      "Epoch 32 Batch 50 Loss 0.0582 Accuracy 0.4972\n",
      "Epoch 32 Batch 100 Loss 0.0571 Accuracy 0.4975\n",
      "Epoch 32 Loss 0.0547 Accuracy 0.4978\n",
      "Epoch 33 Batch 0 Loss 0.0627 Accuracy 0.5039\n",
      "Epoch 33 Batch 50 Loss 0.0458 Accuracy 0.4976\n",
      "Epoch 33 Batch 100 Loss 0.0524 Accuracy 0.4981\n",
      "Epoch 33 Loss 0.0505 Accuracy 0.4982\n",
      "Epoch 34 Batch 0 Loss 0.0419 Accuracy 0.5039\n",
      "Epoch 34 Batch 50 Loss 0.0548 Accuracy 0.4947\n",
      "Epoch 34 Batch 100 Loss 0.0604 Accuracy 0.4951\n",
      "Epoch 34 Loss 0.0571 Accuracy 0.4960\n",
      "Epoch 35 Batch 0 Loss 0.0481 Accuracy 0.5039\n",
      "Epoch 35 Batch 50 Loss 0.0456 Accuracy 0.4972\n",
      "Epoch 35 Batch 100 Loss 0.0570 Accuracy 0.4965\n",
      "Epoch 35 Loss 0.0577 Accuracy 0.4963\n",
      "Epoch 36 Batch 0 Loss 0.0667 Accuracy 0.5000\n",
      "Epoch 36 Batch 50 Loss 0.0518 Accuracy 0.4949\n",
      "Epoch 36 Batch 100 Loss 0.0553 Accuracy 0.4945\n",
      "Epoch 36 Loss 0.0536 Accuracy 0.4946\n",
      "Epoch 37 Batch 0 Loss 0.0672 Accuracy 0.5000\n",
      "Epoch 37 Batch 50 Loss 0.0555 Accuracy 0.4948\n",
      "Epoch 37 Batch 100 Loss 0.0561 Accuracy 0.4949\n",
      "Epoch 37 Loss 0.0543 Accuracy 0.4949\n",
      "Epoch 38 Batch 0 Loss 0.0648 Accuracy 0.4961\n",
      "Epoch 38 Batch 50 Loss 0.0514 Accuracy 0.4945\n",
      "Epoch 38 Batch 100 Loss 0.0537 Accuracy 0.4948\n",
      "Epoch 38 Loss 0.0520 Accuracy 0.4950\n",
      "Epoch 39 Batch 0 Loss 0.0652 Accuracy 0.5000\n",
      "Epoch 39 Batch 50 Loss 0.0468 Accuracy 0.4947\n",
      "Epoch 39 Batch 100 Loss 0.0545 Accuracy 0.4951\n",
      "Epoch 39 Loss 0.0529 Accuracy 0.4952\n",
      "Epoch 40 Batch 0 Loss 0.0565 Accuracy 0.5000\n",
      "Epoch 40 Batch 50 Loss 0.0507 Accuracy 0.4952\n",
      "Epoch 40 Batch 100 Loss 0.0550 Accuracy 0.4949\n",
      "Epoch 40 Loss 0.0547 Accuracy 0.4950\n",
      "Epoch 41 Batch 0 Loss 0.0676 Accuracy 0.4961\n",
      "Epoch 41 Batch 50 Loss 0.0496 Accuracy 0.4946\n",
      "Epoch 41 Batch 100 Loss 0.0534 Accuracy 0.4950\n",
      "Epoch 41 Loss 0.0519 Accuracy 0.4952\n",
      "Epoch 42 Batch 0 Loss 0.0727 Accuracy 0.5000\n",
      "Epoch 42 Batch 50 Loss 0.0480 Accuracy 0.4953\n",
      "Epoch 42 Batch 100 Loss 0.0531 Accuracy 0.4952\n",
      "Epoch 42 Loss 0.0527 Accuracy 0.4951\n",
      "Epoch 43 Batch 0 Loss 0.0637 Accuracy 0.5000\n",
      "Epoch 43 Batch 50 Loss 0.0544 Accuracy 0.4952\n",
      "Epoch 43 Batch 100 Loss 0.0560 Accuracy 0.4957\n",
      "Epoch 43 Loss 0.0559 Accuracy 0.4955\n",
      "Epoch 44 Batch 0 Loss 0.0880 Accuracy 0.4883\n",
      "Epoch 44 Batch 50 Loss 0.0607 Accuracy 0.4931\n",
      "Epoch 44 Batch 100 Loss 0.0636 Accuracy 0.4938\n",
      "Epoch 44 Loss 0.0637 Accuracy 0.4931\n",
      "Epoch 45 Batch 0 Loss 0.0914 Accuracy 0.5000\n",
      "Epoch 45 Batch 50 Loss 0.0634 Accuracy 0.4923\n",
      "Epoch 45 Batch 100 Loss 0.0623 Accuracy 0.4930\n",
      "Epoch 45 Loss 0.0594 Accuracy 0.4937\n",
      "Epoch 46 Batch 0 Loss 0.0579 Accuracy 0.5000\n",
      "Epoch 46 Batch 50 Loss 0.0513 Accuracy 0.4952\n",
      "Epoch 46 Batch 100 Loss 0.0558 Accuracy 0.4949\n",
      "Epoch 46 Loss 0.0545 Accuracy 0.4948\n",
      "Epoch 47 Batch 0 Loss 0.0667 Accuracy 0.5000\n",
      "Epoch 47 Batch 50 Loss 0.0543 Accuracy 0.4949\n",
      "Epoch 47 Batch 100 Loss 0.0581 Accuracy 0.4944\n",
      "Epoch 47 Loss 0.0580 Accuracy 0.4945\n",
      "Epoch 48 Batch 0 Loss 0.0608 Accuracy 0.5039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Batch 50 Loss 0.0616 Accuracy 0.4943\n",
      "Epoch 48 Batch 100 Loss 0.0621 Accuracy 0.4939\n",
      "Epoch 48 Loss 0.0632 Accuracy 0.4936\n",
      "Epoch 49 Batch 0 Loss 0.0922 Accuracy 0.4922\n",
      "Epoch 49 Batch 50 Loss 0.0570 Accuracy 0.4942\n",
      "Epoch 49 Batch 100 Loss 0.0572 Accuracy 0.4941\n",
      "Epoch 49 Loss 0.0559 Accuracy 0.4942\n",
      "Epoch 50 Batch 0 Loss 0.0638 Accuracy 0.5000\n",
      "Epoch 50 Batch 50 Loss 0.4533 Accuracy 0.4242\n",
      "Epoch 50 Batch 100 Loss 0.6346 Accuracy 0.3389\n",
      "Epoch 50 Loss 0.6619 Accuracy 0.3214\n",
      "Epoch 51 Batch 0 Loss 0.8008 Accuracy 0.2578\n",
      "Epoch 51 Batch 50 Loss 0.7697 Accuracy 0.2618\n",
      "Epoch 51 Batch 100 Loss 0.6931 Accuracy 0.2982\n",
      "Epoch 51 Loss 0.5887 Accuracy 0.3315\n",
      "Epoch 52 Batch 0 Loss 0.1307 Accuracy 0.4805\n",
      "Epoch 52 Batch 50 Loss 0.1522 Accuracy 0.4681\n",
      "Epoch 52 Batch 100 Loss 0.1510 Accuracy 0.4691\n",
      "Epoch 52 Loss 0.1454 Accuracy 0.4705\n",
      "Epoch 53 Batch 0 Loss 0.1158 Accuracy 0.4922\n",
      "Epoch 53 Batch 50 Loss 0.1370 Accuracy 0.4725\n",
      "Epoch 53 Batch 100 Loss 0.1349 Accuracy 0.4730\n",
      "Epoch 53 Loss 0.1299 Accuracy 0.4740\n",
      "Epoch 54 Batch 0 Loss 0.1374 Accuracy 0.4805\n",
      "Epoch 54 Batch 50 Loss 0.0983 Accuracy 0.4844\n",
      "Epoch 54 Batch 100 Loss 0.0768 Accuracy 0.4899\n",
      "Epoch 54 Loss 0.0708 Accuracy 0.4910\n",
      "Epoch 55 Batch 0 Loss 0.0556 Accuracy 0.5000\n",
      "Epoch 55 Batch 50 Loss 0.0521 Accuracy 0.4950\n",
      "Epoch 55 Batch 100 Loss 0.0533 Accuracy 0.4952\n",
      "Epoch 55 Loss 0.0528 Accuracy 0.4954\n",
      "Epoch 56 Batch 0 Loss 0.0599 Accuracy 0.5000\n",
      "Epoch 56 Batch 50 Loss 0.0504 Accuracy 0.4953\n",
      "Epoch 56 Batch 100 Loss 0.0529 Accuracy 0.4951\n",
      "Epoch 56 Loss 0.0524 Accuracy 0.4952\n",
      "Epoch 57 Batch 0 Loss 0.0696 Accuracy 0.5000\n",
      "Epoch 57 Batch 50 Loss 0.0470 Accuracy 0.4966\n",
      "Epoch 57 Batch 100 Loss 0.0519 Accuracy 0.4959\n",
      "Epoch 57 Loss 0.0524 Accuracy 0.4957\n",
      "Epoch 58 Batch 0 Loss 0.0637 Accuracy 0.5000\n",
      "Epoch 58 Batch 50 Loss 0.0516 Accuracy 0.4948\n",
      "Epoch 58 Batch 100 Loss 0.0558 Accuracy 0.4950\n",
      "Epoch 58 Loss 0.0546 Accuracy 0.4950\n",
      "Epoch 59 Batch 0 Loss 0.0616 Accuracy 0.5000\n",
      "Epoch 59 Batch 50 Loss 0.0528 Accuracy 0.4952\n",
      "Epoch 59 Batch 100 Loss 0.0567 Accuracy 0.4949\n",
      "Epoch 59 Loss 0.0549 Accuracy 0.4951\n",
      "Epoch 60 Batch 0 Loss 0.0755 Accuracy 0.5000\n",
      "Epoch 60 Batch 50 Loss 0.0507 Accuracy 0.4950\n",
      "Epoch 60 Batch 100 Loss 0.0548 Accuracy 0.4946\n",
      "Epoch 60 Loss 0.0539 Accuracy 0.4947\n",
      "Epoch 61 Batch 0 Loss 0.0714 Accuracy 0.5000\n",
      "Epoch 61 Batch 50 Loss 0.0483 Accuracy 0.4959\n",
      "Epoch 61 Batch 100 Loss 0.0555 Accuracy 0.4956\n",
      "Epoch 61 Loss 0.0544 Accuracy 0.4955\n",
      "Epoch 62 Batch 0 Loss 0.0738 Accuracy 0.4961\n",
      "Epoch 62 Batch 50 Loss 0.0473 Accuracy 0.4953\n",
      "Epoch 62 Batch 100 Loss 0.0521 Accuracy 0.4956\n",
      "Epoch 62 Loss 0.0774 Accuracy 0.4918\n",
      "Epoch 63 Batch 0 Loss 0.1348 Accuracy 0.4766\n",
      "Epoch 63 Batch 50 Loss 0.1140 Accuracy 0.4781\n",
      "Epoch 63 Batch 100 Loss 0.1158 Accuracy 0.4797\n",
      "Epoch 63 Loss 0.1097 Accuracy 0.4813\n",
      "Epoch 64 Batch 0 Loss 0.0782 Accuracy 0.5039\n",
      "Epoch 64 Batch 50 Loss 0.0836 Accuracy 0.4872\n",
      "Epoch 64 Batch 100 Loss 0.0804 Accuracy 0.4898\n",
      "Epoch 64 Loss 0.0830 Accuracy 0.4895\n",
      "Epoch 65 Batch 0 Loss 0.0866 Accuracy 0.4961\n",
      "Epoch 65 Batch 50 Loss 0.0630 Accuracy 0.4957\n",
      "Epoch 65 Batch 100 Loss 0.0659 Accuracy 0.4954\n",
      "Epoch 65 Loss 0.0620 Accuracy 0.4958\n",
      "Epoch 66 Batch 0 Loss 0.0573 Accuracy 0.5039\n",
      "Epoch 66 Batch 50 Loss 0.0591 Accuracy 0.4948\n",
      "Epoch 66 Batch 100 Loss 0.0603 Accuracy 0.4955\n",
      "Epoch 66 Loss 0.0564 Accuracy 0.4959\n",
      "Epoch 67 Batch 0 Loss 0.0570 Accuracy 0.5039\n",
      "Epoch 67 Batch 50 Loss 0.0601 Accuracy 0.4957\n",
      "Epoch 67 Batch 100 Loss 0.0596 Accuracy 0.4966\n",
      "Epoch 67 Loss 0.0571 Accuracy 0.4967\n",
      "Epoch 68 Batch 0 Loss 0.0502 Accuracy 0.5039\n",
      "Epoch 68 Batch 50 Loss 0.0545 Accuracy 0.4965\n",
      "Epoch 68 Batch 100 Loss 0.0567 Accuracy 0.4961\n",
      "Epoch 68 Loss 0.0528 Accuracy 0.4967\n",
      "Epoch 69 Batch 0 Loss 0.0625 Accuracy 0.5039\n",
      "Epoch 69 Batch 50 Loss 0.0505 Accuracy 0.4969\n",
      "Epoch 69 Batch 100 Loss 0.0524 Accuracy 0.4974\n",
      "Epoch 69 Loss 0.0500 Accuracy 0.4975\n",
      "Epoch 70 Batch 0 Loss 0.0498 Accuracy 0.5039\n",
      "Epoch 70 Batch 50 Loss 0.0511 Accuracy 0.4965\n",
      "Epoch 70 Batch 100 Loss 0.0479 Accuracy 0.4975\n",
      "Epoch 70 Loss 0.0485 Accuracy 0.4971\n",
      "Epoch 71 Batch 0 Loss 0.0619 Accuracy 0.5000\n",
      "Epoch 71 Batch 50 Loss 0.0474 Accuracy 0.4966\n",
      "Epoch 71 Batch 100 Loss 0.0449 Accuracy 0.4980\n",
      "Epoch 71 Loss 0.0428 Accuracy 0.4981\n",
      "Epoch 72 Batch 0 Loss 0.0925 Accuracy 0.5000\n",
      "Epoch 72 Batch 50 Loss 0.0505 Accuracy 0.4962\n",
      "Epoch 72 Batch 100 Loss 0.0486 Accuracy 0.4973\n",
      "Epoch 72 Loss 0.0463 Accuracy 0.4975\n",
      "Epoch 73 Batch 0 Loss 0.0591 Accuracy 0.5039\n",
      "Epoch 73 Batch 50 Loss 0.0417 Accuracy 0.4981\n",
      "Epoch 73 Batch 100 Loss 0.0407 Accuracy 0.4988\n",
      "Epoch 73 Loss 0.0400 Accuracy 0.4987\n",
      "Epoch 74 Batch 0 Loss 0.0976 Accuracy 0.5000\n",
      "Epoch 74 Batch 50 Loss 0.0431 Accuracy 0.4972\n",
      "Epoch 74 Batch 100 Loss 0.0424 Accuracy 0.4984\n",
      "Epoch 74 Loss 0.0463 Accuracy 0.4980\n",
      "Epoch 75 Batch 0 Loss 0.0557 Accuracy 0.5039\n",
      "Epoch 75 Batch 50 Loss 0.0416 Accuracy 0.4974\n",
      "Epoch 75 Batch 100 Loss 0.0393 Accuracy 0.4984\n",
      "Epoch 75 Loss 0.0375 Accuracy 0.4984\n",
      "Epoch 76 Batch 0 Loss 0.0485 Accuracy 0.5039\n",
      "Epoch 76 Batch 50 Loss 0.0399 Accuracy 0.4975\n",
      "Epoch 76 Batch 100 Loss 0.0444 Accuracy 0.4976\n",
      "Epoch 76 Loss 0.0428 Accuracy 0.4977\n",
      "Epoch 77 Batch 0 Loss 0.0488 Accuracy 0.5039\n",
      "Epoch 77 Batch 50 Loss 0.0453 Accuracy 0.4975\n",
      "Epoch 77 Batch 100 Loss 0.0559 Accuracy 0.4965\n",
      "Epoch 77 Loss 0.0539 Accuracy 0.4966\n",
      "Epoch 78 Batch 0 Loss 0.0540 Accuracy 0.5039\n",
      "Epoch 78 Batch 50 Loss 0.0357 Accuracy 0.4982\n",
      "Epoch 78 Batch 100 Loss 0.0392 Accuracy 0.4987\n",
      "Epoch 78 Loss 0.0374 Accuracy 0.4987\n",
      "Epoch 79 Batch 0 Loss 0.0412 Accuracy 0.5039\n",
      "Epoch 79 Batch 50 Loss 0.0387 Accuracy 0.4977\n",
      "Epoch 79 Batch 100 Loss 0.0408 Accuracy 0.4982\n",
      "Epoch 79 Loss 0.0403 Accuracy 0.4981\n",
      "Epoch 80 Batch 0 Loss 0.0672 Accuracy 0.5000\n",
      "Epoch 80 Batch 50 Loss 0.0489 Accuracy 0.4969\n",
      "Epoch 80 Batch 100 Loss 0.0466 Accuracy 0.4976\n",
      "Epoch 80 Loss 0.0441 Accuracy 0.4978\n",
      "Epoch 81 Batch 0 Loss 0.0474 Accuracy 0.5039\n",
      "Epoch 81 Batch 50 Loss 0.0439 Accuracy 0.4969\n",
      "Epoch 81 Batch 100 Loss 0.0397 Accuracy 0.4980\n",
      "Epoch 81 Loss 0.0391 Accuracy 0.4981\n",
      "Epoch 82 Batch 0 Loss 0.0977 Accuracy 0.4961\n",
      "Epoch 82 Batch 50 Loss 0.0755 Accuracy 0.4939\n",
      "Epoch 82 Batch 100 Loss 0.0675 Accuracy 0.4957\n",
      "Epoch 82 Loss 0.0614 Accuracy 0.4961\n",
      "Epoch 83 Batch 0 Loss 0.0498 Accuracy 0.5039\n",
      "Epoch 83 Batch 50 Loss 0.0410 Accuracy 0.4980\n",
      "Epoch 83 Batch 100 Loss 0.0388 Accuracy 0.4986\n",
      "Epoch 83 Loss 0.0373 Accuracy 0.4987\n",
      "Epoch 84 Batch 0 Loss 0.0357 Accuracy 0.5000\n",
      "Epoch 84 Batch 50 Loss 0.0482 Accuracy 0.4963\n",
      "Epoch 84 Batch 100 Loss 0.0415 Accuracy 0.4979\n",
      "Epoch 84 Loss 0.0391 Accuracy 0.4981\n",
      "Epoch 85 Batch 0 Loss 0.0910 Accuracy 0.4961\n",
      "Epoch 85 Batch 50 Loss 0.0408 Accuracy 0.4976\n",
      "Epoch 85 Batch 100 Loss 0.0404 Accuracy 0.4984\n",
      "Epoch 85 Loss 0.0384 Accuracy 0.4983\n",
      "Epoch 86 Batch 0 Loss 0.0587 Accuracy 0.5000\n",
      "Epoch 86 Batch 50 Loss 0.0410 Accuracy 0.4975\n",
      "Epoch 86 Batch 100 Loss 0.0425 Accuracy 0.4985\n",
      "Epoch 86 Loss 0.0400 Accuracy 0.4986\n",
      "Epoch 87 Batch 0 Loss 0.0493 Accuracy 0.5039\n",
      "Epoch 87 Batch 50 Loss 0.0366 Accuracy 0.4978\n",
      "Epoch 87 Batch 100 Loss 0.0449 Accuracy 0.4970\n",
      "Epoch 87 Loss 0.0431 Accuracy 0.4973\n",
      "Epoch 88 Batch 0 Loss 0.0568 Accuracy 0.5039\n",
      "Epoch 88 Batch 50 Loss 0.0368 Accuracy 0.4985\n",
      "Epoch 88 Batch 100 Loss 0.0368 Accuracy 0.4991\n",
      "Epoch 88 Loss 0.0355 Accuracy 0.4991\n",
      "Epoch 89 Batch 0 Loss 0.0537 Accuracy 0.5039\n",
      "Epoch 89 Batch 50 Loss 0.0399 Accuracy 0.4969\n",
      "Epoch 89 Batch 100 Loss 0.0386 Accuracy 0.4980\n",
      "Epoch 89 Loss 0.0364 Accuracy 0.4982\n",
      "Epoch 90 Batch 0 Loss 0.0568 Accuracy 0.5000\n",
      "Epoch 90 Batch 50 Loss 0.0375 Accuracy 0.4978\n",
      "Epoch 90 Batch 100 Loss 0.0377 Accuracy 0.4983\n",
      "Epoch 90 Loss 0.0360 Accuracy 0.4983\n",
      "Epoch 91 Batch 0 Loss 0.0579 Accuracy 0.5039\n",
      "Epoch 91 Batch 50 Loss 0.0340 Accuracy 0.4985\n",
      "Epoch 91 Batch 100 Loss 0.0436 Accuracy 0.4980\n",
      "Epoch 91 Loss 0.0428 Accuracy 0.4980\n",
      "Epoch 92 Batch 0 Loss 0.0629 Accuracy 0.5000\n",
      "Epoch 92 Batch 50 Loss 0.0411 Accuracy 0.4975\n",
      "Epoch 92 Batch 100 Loss 0.0427 Accuracy 0.4980\n",
      "Epoch 92 Loss 0.0410 Accuracy 0.4982\n",
      "Epoch 93 Batch 0 Loss 0.0385 Accuracy 0.5000\n",
      "Epoch 93 Batch 50 Loss 0.0448 Accuracy 0.4968\n",
      "Epoch 93 Batch 100 Loss 0.0402 Accuracy 0.4981\n",
      "Epoch 93 Loss 0.0385 Accuracy 0.4982\n",
      "Epoch 94 Batch 0 Loss 0.0549 Accuracy 0.5039\n",
      "Epoch 94 Batch 50 Loss 0.0357 Accuracy 0.4980\n",
      "Epoch 94 Batch 100 Loss 0.0352 Accuracy 0.4989\n",
      "Epoch 94 Loss 0.0345 Accuracy 0.4988\n",
      "Epoch 95 Batch 0 Loss 0.0509 Accuracy 0.4961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 Batch 50 Loss 0.0387 Accuracy 0.4981\n",
      "Epoch 95 Batch 100 Loss 0.0377 Accuracy 0.4987\n",
      "Epoch 95 Loss 0.0378 Accuracy 0.4985\n",
      "Epoch 96 Batch 0 Loss 0.0659 Accuracy 0.5039\n",
      "Epoch 96 Batch 50 Loss 0.0360 Accuracy 0.4977\n",
      "Epoch 96 Batch 100 Loss 0.0334 Accuracy 0.4988\n",
      "Epoch 96 Loss 0.0321 Accuracy 0.4989\n",
      "Epoch 97 Batch 0 Loss 0.1026 Accuracy 0.5000\n",
      "Epoch 97 Batch 50 Loss 0.0344 Accuracy 0.4986\n",
      "Epoch 97 Batch 100 Loss 0.0380 Accuracy 0.4987\n",
      "Epoch 97 Loss 0.0404 Accuracy 0.4981\n",
      "Epoch 98 Batch 0 Loss 0.0739 Accuracy 0.5000\n",
      "Epoch 98 Batch 50 Loss 0.0376 Accuracy 0.4978\n",
      "Epoch 98 Batch 100 Loss 0.0369 Accuracy 0.4985\n",
      "Epoch 98 Loss 0.0348 Accuracy 0.4985\n",
      "Epoch 99 Batch 0 Loss 0.0360 Accuracy 0.5039\n",
      "Epoch 99 Batch 50 Loss 0.0341 Accuracy 0.4985\n",
      "Epoch 99 Batch 100 Loss 0.0358 Accuracy 0.4991\n",
      "Epoch 99 Loss 0.0338 Accuracy 0.4992\n",
      "Epoch 100 Batch 0 Loss 0.0640 Accuracy 0.5000\n",
      "Epoch 100 Batch 50 Loss 0.0308 Accuracy 0.4989\n",
      "Epoch 100 Batch 100 Loss 0.0325 Accuracy 0.4991\n",
      "Epoch 100 Loss 0.0323 Accuracy 0.4991\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        train_step(inp, tar)\n",
    "        if batch % 50 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "              epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: good morning.\n",
      "Predicted translation: ['<start>', '.', '<end>', '<end>', '<end>', '<end>']\n"
     ]
    }
   ],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [1]\n",
    "    end_token = [2]\n",
    "  \n",
    "    sentence = preprocess_sentence(inp_sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    \n",
    "    encoder_input = tf.expand_dims(inputs, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "    decoder_input = [1]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(max_length_targ):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "        if predicted_id == targ_lang_tokenizer.word_index[\"<end>\"]:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "\n",
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "    sentence = inp_lang_tokenizer.encode(sentence)\n",
    "  \n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # plot the attention weights\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "    predicted_sentence = ([targ_lang_tokenizer.index_word[i] for i in result.numpy()])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)\n",
    "        \n",
    "translate(\"good morning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
